{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR-in-Arabic_Lab8-Transformers_&_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day8/IR_in_Arabic_Lab8_Transformers_%26_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **IR in Arabic** - Summer 2021 lab **Day 8**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. This lab aims to build basic practical knowledge on how to use BERT transformer model.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "1. Understand what is huggingface and how to use it.\n",
        "2. Learn how to tokenize a sentence and convert an input sentence to the required format for BERT (deal with special tokens, sentence length & Attention Mask)\n",
        "3. Perform tokenization on a given dataset.\n",
        "4. Check the architecture of each layer in BERT in practice.\n",
        "5. Get to know the output of BERT\n",
        "6. Utilize BERT embedding in computing cosine similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vSBU5h9U_C"
      },
      "source": [
        "## Utilize the GPU of Colab\n",
        "In this session, we will work on experiments that require GPU to run. To make the experiments running over the GPU provided by Colab, you need to do the following:\n",
        "\n",
        "1. Go to Menu > Runtime > Change runtime.\n",
        "\n",
        "2. Change hardware acceleration to GPU.\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0I7C40zV12G"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# Choose GPU as device to run the experiments on \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "## **Hugging Face** \n",
        "[Hugging face](https://huggingface.co/) is an NLP-focused startup with a large open-source community, in particular around the Transformers library. ü§ó Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the pip package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU"
      },
      "source": [
        "#install the transformer library\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmC0oRLO_2c6"
      },
      "source": [
        "# install needed libraries\n",
        "#install the Pyterrier framework\n",
        "!pip install python-terrier\n",
        "#install the Arabic stop words library\n",
        "!pip install Arabic-Stopwords\n",
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import re\n",
        "from snowballstemmer import stemmer\n",
        "from tqdm import tqdm\n",
        "import arabicstopwords.arabicstopwords as stp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIzygfXzAILT"
      },
      "source": [
        "### Transformer Components in Hugging face\n",
        "Transformers is based around the concept of pre-trained transformer models. These transformer models come in different shapes, sizes, and architectures and have their own ways of accepting input data: via tokenization.\n",
        "\n",
        "The library builds on three main classes:\n",
        "1. **The configuration class:** hosts relevant information concerning the model we will be using, such as the number of layers and the number of attention heads. Below is an example of a BERT configuration file, for the pre-trained weights bert-base-cased. The configuration classes host these attributes with various I/O methods and standardized name properties.\n",
        "\n",
        "{\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 28996\n",
        "}\n",
        "\n",
        "2. **The tokenizer class:** the tokenizer class takes care of converting python string in arrays or tensors of integers which are indices in a model vocabulary. It has many handy features revolving around the tokenization of a string into tokens. This tokenization varies according to the model, therefore each model has its own tokenizer.\n",
        "3. **The model class:** the model class holds the neural network modeling logic itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfT-xpdo8WBv"
      },
      "source": [
        "## Loading an Arabic BERT model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTqmkPD8cib"
      },
      "source": [
        "In the following example, we show how to load an Arabic BERT model which is AraBERT. AraBERT is one of the famous models in literature and performs well for many Arabic tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv01\"\n",
        "\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "arabert_model = AutoModel.from_pretrained(model_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYVmK5WOD-"
      },
      "source": [
        "## **Perform tokenization**\n",
        "\n",
        "To feed our text to BERT, it must be splitted into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT tokenizer. Now, we utilize the pretrained AraBERT model to tokenize a given sentence. We just need to provide the sentence as an input string to the loaded tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIEdqNl_zDO"
      },
      "source": [
        "## Example 1 of tokenization\n",
        "text1 = \"Ÿáÿ∞ÿß ŸáŸà ÿßŸÑŸäŸàŸÖ ÿßŸÑÿ™ÿßÿ≥ÿπ ŸÖŸÜ ÿØŸàÿ±ÿ© ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ÿØÿ±ÿ≥ ÿßŸÑŸäŸàŸÖ ŸÖŸáŸÖ ÿ¨ÿØÿß ŸÅŸä ŸÖÿ¨ÿßŸÑ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÑÿ∫ÿßÿ™ ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©\"\n",
        "tokeninzed_text1 = arabert_tokenizer.tokenize(text1)\n",
        "text1_token_ids = arabert_tokenizer.convert_tokens_to_ids(tokeninzed_text1)\n",
        "\n",
        "# Print the original sentence.\n",
        "print('Original text1: ', text1)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized text1 : ', tokeninzed_text1)\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs of text1: ',text1_token_ids )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkECQs4BXl3N"
      },
      "source": [
        "# Example 2 of tokenization \n",
        "# This example should be completed by students \n",
        "text2 = \"ÿ≥ŸÜÿ™ÿπŸÑŸÖ ÿßŸÑŸäŸàŸÖ ŸÉŸäŸÅ ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿ±ÿ™ ÿ®ÿ¥ŸÉŸÑ ÿπŸÖŸÑŸä ŸàŸÜÿ¨ÿ±Ÿä ÿ®ÿπÿ∂ ÿßŸÑÿ™ÿ¨ÿßÿ±ÿ®\"\n",
        "\n",
        "# The remaining code of this cell should be deleted as students require to write it \n",
        "tokeninzed_text2 = # student should complete this line on chats\n",
        "text2_token_ids = # student should complete this line on chats\n",
        "\n",
        "# Print the original sentence.\n",
        "print('Original text2: ', text2)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized text2 : ', tokeninzed_text2)\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs of text2: ',text2_token_ids )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyKXDJf0p_Lp"
      },
      "source": [
        "## **BERT Required Formatting**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-qA3T9qoBs"
      },
      "source": [
        "The input for BERT model has to be in specific format. What we need is to:\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length. Maximum allowed length is 512.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-dYLJEaqcl"
      },
      "source": [
        "<!-- **`[SEP]`** -->\n",
        "\n",
        "At the end of every sentence, we need to append the special **`[SEP]`** token. \n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to do some task on them (e.g., can the answer to the question in sentence A be found in sentence B?). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He4MI90SbJHg"
      },
      "source": [
        "sep_token =arabert_tokenizer.sep_token\n",
        "\n",
        "# print sep token of the tokenizer\n",
        "print(\"Sep token : \", sep_token)\n",
        "\n",
        "# print the token id of sep token\n",
        "print('Token ID of sep token : ',  arabert_tokenizer.convert_tokens_to_ids(sep_token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeI0dOoPbD_a"
      },
      "source": [
        "<!-- **`[CLS]`** -->\n",
        "\n",
        ">  \"The first token of every sequence is always a special classification token (**`[CLS]`**). The final hidden state\n",
        "corresponding to this token is used as the aggregate sequence representation for classification\n",
        "tasks.\" ([BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n",
        "\n",
        "On the output of the final (12th) transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier.\n",
        "\n",
        "![Illustration of CLS token purpose](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLmvDiTq6oW"
      },
      "source": [
        "cls_token =arabert_tokenizer.cls_token\n",
        "\n",
        "# print cls token of the tokenizer\n",
        "print(\"Cls token : \", cls_token)\n",
        "\n",
        "# print the token id of cls token\n",
        "print('Token ID of cls token : ',  arabert_tokenizer.convert_tokens_to_ids(cls_token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dkV6D4rgOI"
      },
      "source": [
        "### **Sentence Length & Attention Mask**\n",
        "\n",
        "BERT has two constraints:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special **`[PAD]`** token, which is at index 0 in the BERT vocabulary. \n",
        "\n",
        "The **\"Attention Mask\"** is simply an array of 1s and 0s indicating which tokens are padding and which aren't. This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
        "\n",
        "The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "The maximum length does impact training and evaluation speed. \n",
        "For example, with a Tesla K80:\n",
        "\n",
        "`MAX_LEN = 128  -->  Training epochs take ~5:28 each`\n",
        "\n",
        "`MAX_LEN = 64   -->  Training epochs take ~2:57 each`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSF2lO71gf_L"
      },
      "source": [
        "### Perform tokenization on a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnQz-uyzrsRd"
      },
      "source": [
        "# `encode_plus` will:\n",
        "#   (1) Tokenize the sentence.\n",
        "#   (2) Prepend the `[CLS]` token to the start.\n",
        "#   (3) Append the `[SEP]` token to the end.\n",
        "#   (4) Map tokens to their IDs.\n",
        "#   (5) Pad or truncate the sentence to `max_length`\n",
        "#   (6) Create attention masks for [PAD] tokens.\n",
        "\n",
        "text = \"ÿ≥ŸÜÿ™ÿπŸÑŸÖ ÿßŸÑŸäŸàŸÖ ŸÉŸäŸÅ ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿ±ÿ™ ÿ®ÿ¥ŸÉŸÑ ÿπŸÖŸÑŸä ŸàŸÜÿ¨ÿ±Ÿä ÿ®ÿπÿ∂ ÿßŸÑÿ™ÿ¨ÿßÿ±ÿ®\"\n",
        "encoding= arabert_tokenizer.encode_plus(\n",
        "                  text,                      # Sentence to encode.\n",
        "                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                  truncation=True,\n",
        "                  max_length = 32,           # Pad & truncate all sentences.\n",
        "                  padding=\"max_length\",\n",
        "                  return_attention_mask = True,   # Construct attention mask\n",
        "                  return_tensors = 'pt',     # Return pytorch tensors.\n",
        "              )\n",
        "\n",
        "    \n",
        "# Print the input ids and attention mask of the encoded sentence\n",
        "print(\"Original text: \", text)\n",
        "print(\"Input ids: \", encoding[\"input_ids\"].flatten(),)\n",
        "print(\"Attention mask: \", encoding[\"attention_mask\"].flatten(),)\n",
        "# Note in the output of the next line that the cls, sep,and pad tokens were added automatically\n",
        "print(\"Tokenized text: \",arabert_tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].flatten()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZY1yWSgr_jj"
      },
      "source": [
        "## **Dataset tokenization**\n",
        "\n",
        "Let's tokenize the EveTAR dataset.\n",
        "First, we need to load the dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjCZeSSsRh2"
      },
      "source": [
        "dataset_links=[\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-01.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-02.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-03.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-04.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-05.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-06.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-07.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-08.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-09.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-10.txt\"]\n",
        "\n",
        "full_data=pd.DataFrame()\n",
        "for i in tqdm(range(len(dataset_links))):\n",
        "    tweets=pd.read_csv(dataset_links[i], sep='\\t')\n",
        "    full_data=pd.concat([full_data,tweets],ignore_index=True)\n",
        "full_data.reset_index(inplace=True,drop=True)\n",
        "full_data  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0IMRifzoCm_"
      },
      "source": [
        "Now, we need to encode each tweet in this dataset.\n",
        "Encoding the dataset is your job now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF1Uc3VroMy4"
      },
      "source": [
        "def encode(text, max_length=32):\n",
        "    return arabert_tokenizer.encode_plus(\n",
        "                  text,                      # Sentence to encode.\n",
        "                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                  truncation=True,\n",
        "                  max_length = max_length,           # Pad & truncate all sentences.\n",
        "                  padding=\"max_length\",\n",
        "                  return_attention_mask = True,   # Construct attention mask\n",
        "                  return_tensors = 'pt',     # Return pytorch tensors.\n",
        "    )\n",
        "\n",
        "tokenized_tweets = []\n",
        "for tweet in tqdm(full_data[\"tweetText\"].values, desc=\"Tokenizing ...\"):\n",
        "    tokenized_tweets.append(encode(tweet, max_length=32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN3bFuTZpjLR"
      },
      "source": [
        "tokenized_tweets[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2mlvWlZhIt6"
      },
      "source": [
        "len(tokenized_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3GGDjXvuyrw"
      },
      "source": [
        "## BERT layers\n",
        "\n",
        "Let's see in practice the layers that consist the BERT model. Simply, you can see every layer in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn8_4baOpqmu"
      },
      "source": [
        "arabert_model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-HvUt_Yedy"
      },
      "source": [
        "## BERT output \n",
        "\n",
        "Let's see what is the output of BERT for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwBC5Rwt8jk"
      },
      "source": [
        "input_ids = tokenized_tweets[0][\"input_ids\"].to(device)\n",
        "attention_mask = tokenized_tweets[0][\"attention_mask\"].to(device)\n",
        "output = arabert_model(input_ids=input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOWnMehpv69i"
      },
      "source": [
        "output[0].shape # batch_size x sequence_length x embedding_dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16v0JoHSw74f"
      },
      "source": [
        " Let's see the embedding vector for the tokens. In this case, we have 32 tokens. Each token has embedding vector of length 768"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHXVqh34wWPn"
      },
      "source": [
        "# print the embedding of all input tokens.\n",
        "all_embeddings = output[0][0]\n",
        "print(all_embeddings.shape)\n",
        "print(all_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlGr-iRxxTb_"
      },
      "source": [
        "# print the cls embedding\n",
        "cls_embedding = output[0][0][0]\n",
        "print(cls_embedding.shape)\n",
        "print(cls_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnGYMFQoxlsT"
      },
      "source": [
        "# print the first token embedding\n",
        "first_token_embedding = output[0][0][1]\n",
        "print(first_token_embedding.shape)\n",
        "print(first_token_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FVdZjX5vHMg"
      },
      "source": [
        "## **Exercise1**\n",
        "Given the following sentences:\n",
        "\n",
        "sent_1 = \"ŸÇÿßŸÖ ŸÅÿ±ŸäŸÇ ÿ®ÿ≠ÿ´Ÿä ŸÅŸä ÿ¨ÿßŸÖÿπÿ© ŸÇÿ∑ÿ± ÿ®ÿßÿ¨ÿ±ÿßÿ° ÿØŸàÿ±ÿ© ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"\n",
        "\n",
        "sent_2 = \"ŸÇÿßŸÖ ÿßŸÑÿ∑ŸÑÿßÿ® ÿßÿ≠ÿ™ÿ±ÿßŸÖÿß ŸÑŸÇÿØŸàŸÖ ÿßÿ≥ÿ™ÿßÿ∞ŸáŸÖ\"\n",
        "\n",
        "1. Compute the embeddings of these sentences\n",
        "2. Check whether the embeddings of the word \"ŸÇÿßŸÖ\" are equal by computing the cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivkp1p_jOsA"
      },
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# helper function \n",
        "def compute_cosine_similarity(a, b):\n",
        "    cos_sim = dot(a, b) / (norm(a) * norm(b))\n",
        "    return cos_sim\n",
        "\n",
        "sent_1 = \"ŸÇÿßŸÖ ŸÅÿ±ŸäŸÇ ÿ®ÿ≠ÿ´Ÿä ŸÅŸä ÿ¨ÿßŸÖÿπÿ© ŸÇÿ∑ÿ± ÿ®ÿßÿ¨ÿ±ÿßÿ° ÿØŸàÿ±ÿ© ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"\n",
        "sent_2 = \"ŸÇÿßŸÖ ÿßŸÑÿ∑ŸÑÿßÿ® ÿßÿ≠ÿ™ÿ±ÿßŸÖÿß ŸÑŸÇÿØŸàŸÖ ÿßÿ≥ÿ™ÿßÿ∞ŸáŸÖ\"\n",
        "\n",
        "# Before passing the embedding to compute cosine similarity score, \n",
        "# you have to convert them from tensor to numpy array as follows:\n",
        "# input1 = token1_embeddging.detach().numpy() \n",
        "# input2 = token2_embeddging.detach().numpy() \n",
        "# cosine_score = compute_cosine_similarity(input1, input2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPtnqf9CwePZ"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Given a list of query-document pairs:\n",
        "1. Compute the cosine similarity for each pair using AraBERT. Store the values as a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiCA05Yiw_Xl"
      },
      "source": [
        "selected_queries = ['E14','E48', 'E36', 'E58', 'E19', 'E63', 'E30', 'E27', 'E39', 'E21']\n",
        "\n",
        "data_path = \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/labs/day8/bm25_top_1000_with_text.txt\"\n",
        "df_data = pd.read_csv(data_path,  sep='\\t',encoding=\"utf-8\")\n",
        "df_data= df_data[0:10]\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJM1iYORZwuk"
      },
      "source": [
        "# write your solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "### **References**\n",
        "\n",
        "\n",
        "*  [Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0](https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html).\n",
        "*   [BERT Fine-Tuning Tutorial with PyTorch.](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "\n"
      ]
    }
  ]
}